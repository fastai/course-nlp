{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is adapted from [this one](https://github.com/fastai/fastai_docs/blob/master/dev_course/dl2/translation_transformer.ipynb) created by Sylvain Gugger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia AI researcher [Chip Huyen](https://huyenchip.com/) wrote a great post [Top 8 trends from ICLR 2019](https://huyenchip.com/2019/05/12/top-8-trends-from-iclr-2019.html) in which one of the trends is that *RNN is losing its luster with researchers*.\n",
    "\n",
    "There's good reason for this, RNNs can be a pain: parallelization can be tricky and they can be difficult to debug. Since language is recursive, it seemed like RNNs were a good conceptual fit with NLP, but recently methods using *attention* have been achieving state of the art results on NLP.\n",
    "\n",
    "This is still an area of very active research, for instance, a recent paper [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430) showed that convolutions can beat attention on some tasks, including English to German translation.  More research is needed on the various strenghts of RNNs, CNNs, and transformers/attention, and perhaps on approaches to combine the best of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/racheltho/.fastai/data/giga-fren/models'),\n",
       " PosixPath('/home/racheltho/.fastai/data/giga-fren/giga-fren.release2.fixed.fr'),\n",
       " PosixPath('/home/racheltho/.fastai/data/giga-fren/cc.en.300.bin'),\n",
       " PosixPath('/home/racheltho/.fastai/data/giga-fren/data_save.pkl'),\n",
       " PosixPath('/home/racheltho/.fastai/data/giga-fren/giga-fren.release2.fixed.en'),\n",
       " PosixPath('/home/racheltho/.fastai/data/giga-fren/cc.fr.300.bin'),\n",
       " PosixPath('/home/racheltho/.fastai/data/giga-fren/questions_easy.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Config().data_path()/'giga-fren'\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reuse the same functions as in the translation notebook to load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding. Flips token order if needed\"\n",
    "    samples = to_data(samples)\n",
    "    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n",
    "    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n",
    "    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n",
    "    if backwards: pad_first = not pad_first\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
    "        else:         \n",
    "            res_x[i,:len(s[0]):],res_y[i,:len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n",
    "    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n",
    "    return res_x, res_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataBunch(TextDataBunch):\n",
    "    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,\n",
    "               pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n",
    "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
    "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
    "        val_bs = ifnone(val_bs, bs)\n",
    "        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
    "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n",
    "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
    "        dataloaders = [train_dl]\n",
    "        for ds in datasets[1:]:\n",
    "            lengths = [len(t) for t in ds.x.items]\n",
    "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
    "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
    "        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTextList(TextList):\n",
    "    _bunch = Seq2SeqDataBunch\n",
    "    _label_cls = TextList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the translation notebook for creation of 'questions_easy.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'questions_easy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = Seq2SeqTextList.from_df(df, path = path, cols='fr').split_by_rand_pct().label_from_df(cols='en', label_cls=TextList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(o) for o in src.train.x.items] + [len(o) for o in src.valid.x.items], 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(o) for o in src.train.y.items] + [len(o) for o in src.valid.y.items], 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we remove questions with more than 30 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = src.filter_by_func(lambda x,y: len(x) > 30 or len(y) > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47389"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src.train) + len(src.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "intercept_args() got an unexpected keyword argument 'dl_tfms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-07789a0b3d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/data_block.py\u001b[0m in \u001b[0;36mdatabunch\u001b[0;34m(self, path, bs, val_bs, num_workers, dl_tfms, device, collate_fn, no_check, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         data = self.x._bunch.create(self.train, self.valid, test_ds=self.test, path=path, bs=bs, val_bs=val_bs,\n\u001b[0;32m--> 547\u001b[0;31m                                     num_workers=num_workers, dl_tfms=dl_tfms, device=device, collate_fn=collate_fn, no_check=no_check, **kwargs)\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'normalize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#In case a normalization was serialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-96e2696c16f2>\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, train_ds, valid_ds, test_ds, path, bs, val_bs, pad_idx, pad_first, device, no_check, backwards, **dl_kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcollate_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq_collate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackwards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSortishSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdl_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: intercept_args() got an unexpected keyword argument 'dl_tfms'"
     ]
    }
   ],
   "source": [
    "data = src.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can load from here when restarting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer model](images/Transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a transform to the dataloader that shifts the targets right and adds a padding at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_tfm(b):\n",
    "    x,y = b\n",
    "    y = F.pad(y, (1, 0), value=1)\n",
    "    return [x,y[:,:-1]], y[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_tfm(shift_tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output embeddings are traditional PyTorch embeddings (and we can use pretrained vectors if we want to). The transformer model isn't a recurrent one, so it has no idea of the relative positions of the words. To help it with that, they had to the input embeddings a positional encoding which is cosine of a certain frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Encode the position with a sinusoid.\"\n",
    "    def __init__(self, d:int):\n",
    "        super().__init__()\n",
    "        self.register_buffer('freq', 1 / (10000 ** (torch.arange(0., d, 2.)/d)))\n",
    "    \n",
    "    def forward(self, pos:Tensor):\n",
    "        inp = torch.ger(pos, self.freq)\n",
    "        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_encoding = PositionalEncoding(20)\n",
    "res = tst_encoding(torch.arange(0,100).float())\n",
    "_, ax = plt.subplots(1,1)\n",
    "for i in range(1,5): ax.plot(res[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"Embedding + positional encoding + dropout\"\n",
    "    def __init__(self, vocab_sz:int, emb_sz:int, inp_p:float=0.):\n",
    "        super().__init__()\n",
    "        self.emb_sz = emb_sz\n",
    "        self.embed = embedding(vocab_sz, emb_sz)\n",
    "        self.pos_enc = PositionalEncoding(emb_sz)\n",
    "        self.drop = nn.Dropout(inp_p)\n",
    "    \n",
    "    def forward(self, inp): \n",
    "        pos = torch.arange(0, inp.size(1), device=inp.device).float()\n",
    "        return self.drop(self.embed(inp) * math.sqrt(self.emb_sz) + self.pos_enc(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward cell is easy: it's just two linear layers with a skip connection and a LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(d_model:int, d_ff:int, ff_p:float=0., double_drop:bool=True):\n",
    "    layers = [nn.Linear(d_model, d_ff), nn.ReLU()]\n",
    "    if double_drop: layers.append(nn.Dropout(ff_p))\n",
    "    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p), MergeLayer(), nn.LayerNorm(d_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multi head attention](images/attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"MutiHeadAttention.\"\n",
    "    \n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int=None, resid_p:float=0., attn_p:float=0., bias:bool=True,\n",
    "                 scale:bool=True):\n",
    "        super().__init__()\n",
    "        d_head = ifnone(d_head, d_model//n_heads)\n",
    "        self.n_heads,self.d_head,self.scale = n_heads,d_head,scale\n",
    "        self.q_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.k_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.v_wgt = nn.Linear(d_model, n_heads * d_head, bias=bias)\n",
    "        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n",
    "        self.drop_att,self.drop_res = nn.Dropout(attn_p),nn.Dropout(resid_p)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        return self.ln(q + self.drop_res(self.out(self._apply_attention(q, k, v, mask=mask))))\n",
    "    \n",
    "    def _apply_attention(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        bs,seq_len = q.size(0),q.size(1)\n",
    "        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        wq,wk,wv = wq.permute(0, 2, 1, 3),wk.permute(0, 2, 3, 1),wv.permute(0, 2, 1, 3)\n",
    "        attn_score = torch.matmul(wq, wk)\n",
    "        if self.scale: attn_score = attn_score.div_(self.d_head ** 0.5)\n",
    "        if mask is not None: \n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n",
    "        attn_vec = torch.matmul(attn_prob, wv)\n",
    "        return attn_vec.permute(0, 2, 1, 3).contiguous().contiguous().view(bs, seq_len, -1)\n",
    "        \n",
    "    def _attention_einsum(self, q:Tensor, k:Tensor, v:Tensor, mask:Tensor=None):\n",
    "        # Permute and matmul is a little bit faster but this implementation is more readable\n",
    "        bs,seq_len = q.size(0),q.size(1)\n",
    "        wq,wk,wv = self.q_wgt(q),self.k_wgt(k),self.v_wgt(v)\n",
    "        wq,wk,wv = map(lambda x:x.view(bs, x.size(1), self.n_heads, self.d_head), (wq,wk,wv))\n",
    "        attn_score = torch.einsum('bind,bjnd->bijn', (wq, wk))\n",
    "        if self.scale: attn_score = attn_score.mul_(1/(self.d_head ** 0.5))\n",
    "        if mask is not None: \n",
    "            attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
    "        attn_prob = self.drop_att(F.softmax(attn_score, dim=2))\n",
    "        attn_vec = torch.einsum('bijn,bjnd->bind', (attn_prob, wv))\n",
    "        return attn_vec.contiguous().view(bs, seq_len, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention layer uses a mask to avoid paying attention to certain timesteps. The first thing is that we don't really want the network to pay attention to the padding, so we're going to mask it. The second thing is that since this model isn't recurrent, we need to mask (in the output) all the tokens we're not supposed to see yet (otherwise it would be cheating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_mask(inp, pad_idx:int=1):\n",
    "    return None\n",
    "    return (inp == pad_idx)[:,None,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_mask(inp, pad_idx:int=1):\n",
    "    return torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None].byte()\n",
    "    return ((inp == pad_idx)[:,None,:,None].long() + torch.triu(inp.new_ones(inp.size(1),inp.size(1)), diagonal=1)[None,None] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of mask for the future tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(10,10), diagonal=1).byte()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to regroup these layers in the blocks we add in the model picture:\n",
    "\n",
    "![Transformer model](images/Transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"Encoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, double_drop:bool=True):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff  = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, x:Tensor, mask:Tensor=None): return self.ff(self.mha(x, x, x, mask=mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"Decoder block of a Transformer model.\"\n",
    "    #Can't use Sequential directly cause more than one input...\n",
    "    def __init__(self, n_heads:int, d_model:int, d_head:int, d_inner:int, resid_p:float=0., attn_p:float=0., ff_p:float=0.,\n",
    "                 bias:bool=True, scale:bool=True, double_drop:bool=True):\n",
    "        super().__init__()\n",
    "        self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, resid_p=resid_p, attn_p=attn_p, bias=bias, scale=scale)\n",
    "        self.ff   = feed_forward(d_model, d_inner, ff_p=ff_p, double_drop=double_drop)\n",
    "    \n",
    "    def forward(self, x:Tensor, enc:Tensor, mask_in:Tensor=None, mask_out:Tensor=None): \n",
    "        y = self.mha1(x, x, x, mask_out)\n",
    "        return self.ff(self.mha2(y, enc, enc, mask=mask_in))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"Transformer model\"\n",
    "    \n",
    "    def __init__(self, inp_vsz:int, out_vsz:int, n_layers:int=6, n_heads:int=8, d_model:int=256, d_head:int=32, \n",
    "                 d_inner:int=1024, inp_p:float=0.1, resid_p:float=0.1, attn_p:float=0.1, ff_p:float=0.1, bias:bool=True, \n",
    "                 scale:bool=True, double_drop:bool=True, pad_idx:int=1):\n",
    "        super().__init__()\n",
    "        self.enc_emb = TransformerEmbedding(inp_vsz, d_model, inp_p)\n",
    "        self.dec_emb = TransformerEmbedding(out_vsz, d_model, 0.)\n",
    "        self.encoder = nn.ModuleList([EncoderBlock(n_heads, d_model, d_head, d_inner, resid_p, attn_p, \n",
    "                                                   ff_p, bias, scale, double_drop) for _ in range(n_layers)])\n",
    "        self.decoder = nn.ModuleList([DecoderBlock(n_heads, d_model, d_head, d_inner, resid_p, attn_p, \n",
    "                                                   ff_p, bias, scale, double_drop) for _ in range(n_layers)])\n",
    "        self.out = nn.Linear(d_model, out_vsz)\n",
    "        self.out.weight = self.dec_emb.embed.weight\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def forward(self, inp, out):\n",
    "        mask_in  = get_padding_mask(inp, self.pad_idx)\n",
    "        mask_out = get_output_mask (out, self.pad_idx)\n",
    "        enc,out = self.enc_emb(inp),self.dec_emb(out)\n",
    "        for enc_block in self.encoder: enc = enc_block(enc, mask_in)\n",
    "        for dec_block in self.decoder: out = dec_block(out, enc, mask_in, mask_out)\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bleu metric (see dedicated notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram():\n",
    "    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "    def __eq__(self, other):\n",
    "        if len(self.ngram) != len(other.ngram): return False\n",
    "        return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "    def __hash__(self): return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grams(x, n, max_n=5000):\n",
    "    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_ngrams(pred, targ, n, max_n=5000):\n",
    "    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n",
    "    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusBLEU(Callback):\n",
    "    def __init__(self, vocab_sz):\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.name = 'bleu'\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.pred_len,self.targ_len,self.corrects,self.counts = 0,0,[0]*4,[0]*4\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        last_output = last_output.argmax(dim=-1)\n",
    "        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "            self.pred_len += len(pred)\n",
    "            self.targ_len += len(targ)\n",
    "            for i in range(4):\n",
    "                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                self.corrects[i] += c\n",
    "                self.counts[i]   += t\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        precs = [c/t for c,t in zip(self.corrects,self.counts)]\n",
    "        len_penalty = exp(1 - self.targ_len/self.pred_len) if self.pred_len < self.targ_len else 1\n",
    "        bleu = len_penalty * ((precs[0]*precs[1]*precs[2]*precs[3]) ** 0.25)\n",
    "        return add_metrics(last_metrics, bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(data.train_ds.x.vocab.itos), len(data.train_ds.y.vocab.itos), d_model=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model, metrics=[accuracy, CorpusBLEU(len(data.train_ds.y.vocab.itos))], \n",
    "                loss_func = CrossEntropyFlat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 13:11 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.424691</td>\n",
       "      <td>2.441312</td>\n",
       "      <td>0.624893</td>\n",
       "      <td>0.465699</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.946387</td>\n",
       "      <td>1.965733</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.513099</td>\n",
       "      <td>01:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.570588</td>\n",
       "      <td>1.628320</td>\n",
       "      <td>0.728240</td>\n",
       "      <td>0.547851</td>\n",
       "      <td>01:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.315216</td>\n",
       "      <td>1.437255</td>\n",
       "      <td>0.753234</td>\n",
       "      <td>0.577680</td>\n",
       "      <td>01:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.128669</td>\n",
       "      <td>1.331126</td>\n",
       "      <td>0.765776</td>\n",
       "      <td>0.591145</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.945706</td>\n",
       "      <td>1.277736</td>\n",
       "      <td>0.776960</td>\n",
       "      <td>0.607566</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.865501</td>\n",
       "      <td>1.265128</td>\n",
       "      <td>0.781391</td>\n",
       "      <td>0.613642</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.759556</td>\n",
       "      <td>1.268143</td>\n",
       "      <td>0.781420</td>\n",
       "      <td>0.614102</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(8, 5e-4, div_factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(learn, ds_type=DatasetType.Valid):\n",
    "    learn.model.eval()\n",
    "    inputs, targets, outputs = [],[],[]\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in progress_bar(learn.dl(ds_type)):\n",
    "            out = learn.model(*xb)\n",
    "            for x,y,z in zip(xb[0],xb[1],out):\n",
    "                inputs.append(learn.data.train_ds.x.reconstruct(x))\n",
    "                targets.append(learn.data.train_ds.y.reconstruct(y))\n",
    "                outputs.append(learn.data.train_ds.y.reconstruct(z.argmax(1)))\n",
    "    return inputs, targets, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='149' class='' max='149', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [149/149 00:26<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, targets, outputs = get_predictions(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj pendant que xxunk les activits requises pour maintenir mon xxunk physique , est - ce que je xxunk de la protection dun rgime dassurance ou de pension ?,\n",
       " Text xxbos xxmaj while i go about maintaining this high degree of fitness , am i protected under an insurance or pension plan ?,\n",
       " Text xxbos xxmaj while i do to the my physical physical of physical , do i aware by the pension plan service plan ?)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[10],targets[10],outputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quelles sont les consquences sur la recherche , la mise en pratique et les politiques en ce qui a trait  l'ac ?,\n",
       " Text xxbos xxmaj what are the xxunk for xxup kt research , practice / policy ?,\n",
       " Text xxbos xxmaj what are the implications implications research kt , , policy and policies in)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[700],targets[700],outputs[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quelle est la position des xxmaj tats - xxmaj unis , du xxmaj canada et de la xxup xxunk  ce propos ?,\n",
       " Text xxbos xxmaj where do the xxup us , xxmaj canada and xxup xxunk stand ?,\n",
       " Text xxbos xxmaj what is xxmaj xxup us xxmaj xxmaj united and the xxunk fit in)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[701],targets[701],outputs[701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quels sont les atouts particuliers du xxmaj canada en recherche sur l'obsit sur la scne internationale ?,\n",
       " Text xxbos xxmaj what are the unique xxmaj canadian strengths in obesity research that set xxmaj canada apart on an international front ?,\n",
       " Text xxbos xxmaj what are xxmaj specific strengths canada strengths in obesity - ? are up canada ? from international international stage ?)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[2500],targets[2500],outputs[2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quelles sont les rpercussions politiques  long terme de cette rvolution scientifique mondiale ?,\n",
       " Text xxbos xxmaj what are some of the long - term policy implications of this global knowledge revolution ?,\n",
       " Text xxbos xxmaj what are the long the long - term policies implications of this global scientific ? ?)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[4002],targets[4002],outputs[4002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They point out in the paper that using label smoothing helped getting a better BLEU/accuracy, even if it made the loss worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(len(data.train_ds.x.vocab.itos), len(data.train_ds.y.vocab.itos), d_model=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(data, model, metrics=[accuracy, CorpusBLEU(len(data.train_ds.y.vocab.itos))], \n",
    "                loss_func=FlattenedLoss(LabelSmoothingCrossEntropy, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 13:27 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.286583</td>\n",
       "      <td>3.337563</td>\n",
       "      <td>0.626293</td>\n",
       "      <td>0.466046</td>\n",
       "      <td>01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.856837</td>\n",
       "      <td>2.906884</td>\n",
       "      <td>0.694047</td>\n",
       "      <td>0.514823</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.625284</td>\n",
       "      <td>2.651707</td>\n",
       "      <td>0.729707</td>\n",
       "      <td>0.548521</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.376412</td>\n",
       "      <td>2.497148</td>\n",
       "      <td>0.751749</td>\n",
       "      <td>0.574652</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.233748</td>\n",
       "      <td>2.400608</td>\n",
       "      <td>0.768083</td>\n",
       "      <td>0.594859</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.103394</td>\n",
       "      <td>2.357367</td>\n",
       "      <td>0.776458</td>\n",
       "      <td>0.604416</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.009131</td>\n",
       "      <td>2.348385</td>\n",
       "      <td>0.781096</td>\n",
       "      <td>0.612397</td>\n",
       "      <td>01:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.913152</td>\n",
       "      <td>2.349686</td>\n",
       "      <td>0.781749</td>\n",
       "      <td>0.612880</td>\n",
       "      <td>01:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(8, 5e-4, div_factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(8, 5e-4, div_factor=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quels sont les atouts particuliers du Canada en recherche sur l'obsit sur la scne internationale ?\n",
      "What are Specific strengths canada strengths in obesity - ? are up canada ? from international international stage ?\n",
      "Quelles sont les rpercussions politiques  long terme de cette rvolution scientifique mondiale ?\n",
      "What are the long the long - term policies implications of this global scientific ? ?\n"
     ]
    }
   ],
   "source": [
    "print(\"Quels sont les atouts particuliers du Canada en recherche sur l'obsit sur la scne internationale ?\")\n",
    "print(\"What are Specific strengths canada strengths in obesity - ? are up canada ? from international international stage ?\")\n",
    "print(\"Quelles sont les rpercussions politiques  long terme de cette rvolution scientifique mondiale ?\")\n",
    "print(\"What are the long the long - term policies implications of this global scientific ? ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quelle distance y a - t - il entre le point le plus rapproch de la surface  xxunk et la position dutilisation habituelle du tube radiogne ?,\n",
       " Text xxbos xxmaj what is the distance between the nearest point of the area to be shielded and the usual operational position of the x - ray tube ?,\n",
       " Text xxbos xxmaj what is the xxmaj between the xxmaj and of the xxmaj ? the ? and the most ? ? of the xxmaj - ray tube ?)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[10],targets[10],outputs[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quels types de prsentations xxmaj sant xxmaj canada xxunk - t - il dans le format ectd  compter du 1er septembre ?,\n",
       " Text xxbos xxmaj what kind of submission types will xxmaj health xxmaj canada accept on xxmaj september 1 , 2004 in ectd format ?,\n",
       " Text xxbos xxmaj what is of information is of be canadian xxmaj canada take ? the canadian ? , and ? the format ?)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[700],targets[700],outputs[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quelles sont les trois caractristiques qui vous incitent le plus  investir dans votre rgion ( xxup nommez - xxup les ) ?,\n",
       " Text xxbos xxmaj what are the three most attractive features about investing in your region ( xxup name xxup it ) ?,\n",
       " Text xxbos xxmaj what is the main main important concerns of the in the country ? xxup xxunk , xxunk ) ?)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[701],targets[701],outputs[701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Text xxbos xxmaj quelles actions avez - vous prises et quel en a t le rsultat ?,\n",
       " Text xxbos xxmaj what were your actions and the outcomes ?,\n",
       " Text xxbos xxmaj what is the targets ? how main of)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[4001],targets[4001],outputs[4001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we change a token in the targets at position n, it shouldn't impact the predictions before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = data.one_batch(cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1,out1 = xb[0][:1],xb[1][:1]\n",
    "inp2,out2 = inp1.clone(),out1.clone()\n",
    "out2[0,15] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = learn.model(inp1, out1)\n",
    "y2 = learn.model(inp2, out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y1[0,:15] - y2[0,:15]).abs().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
